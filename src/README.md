### ALL PYTHON CODE

- `codes.py` - contains all keyterms/regular expressions used for labeling. Contains some terms pulled from RCDC. Make sure those files are contained in a respective RCDC folder. To pull excel file of existing term sets and ontologies being used, look inside folder `term_sets` and use function `read_excel`. Note that the function mainly loads the RCDC files including the term sets into dataframes we can use to pull these terms. We only preserve those terms with weight of 100 to increase accuracy. I'm not sure how to incorporate terms with weights less than 100. I think the smaller the regular expressions/ontologies, the better--we need to focus on the most significant words; quality > quantity. 
   - This file needs to be edited to include the science type codes from the `term_sets/Science_Type` folder.

- `preprocessing.py` - code to pre-process text; remove stop words, sentence filter, etc. current dataset files are saved in `cleaned_data` folder. Merges together pain + HEAL datasets, saves in  `cleaned_data` folder. Note this includes the code to create columns 'Combined Cleaned' for the text and 'Filtered Cleaned,' which utilizes the sentence wrapper searching for aim/objective centered words. Also prepares the dependent variable label columns for machine learning by converting science-type lists to individual labels of binary 1s/0s.  

- `text_utils.py` - contains various functions used in pre-processing, hyper-parameter tuning for the models, data visualizations, sentence wrapper, etc. Look at comments above each function for more details. 

- `key_term_search.py` - NLP, rule-based approaches to search for and count key terms to label. Label predictions saved to `predictions_NLP` under `results/'  
   - This file **needs work** on the science-type classifiers. It is a very **rough base/draft** of what the future code **could** look like.
   - **The science-type functions HAVE NOT been debugged.**
   - I think the regular expressions being used to classify milestone projects as Y/N need work. There's not nearly enough terms being leveraged.
   
- `supervised_models.py` - supervised machine learning approaches to automate labeling. Includes various pipelines ex. Random Forest, KNN, SVM, Logistic Regression. Each have their own functions. Utilized TF-IDF to convert words to feature vectors. Performed dimensionality reduction on matrix using Truncated SVD; used SMOTE to oversample minority labels. Label predictions saved to `predictions_ML` under `results/`. Remember when using on new datasets to train using entire HEAL dataset (all 956 studies)--right now, for testing purposes, it is only being trained on 75% of the data.
  - May need to control-c (^C) terminal and re-run if terminal freezes up.  
 
  - **PLEASE** take a look at how we oversampled using **SMOTE** (Synthetic Minority Oversampling Technique) for categories OUD and Pain under primary outcome. One of our  limitations during this project were that we didn't have enough data for the classifier to accurately learn what type of text corresponds to those labels--thus, we had to use SMOTE to oversample existing data and train the classifier that way. SMOTE doesn't create real text documents, though; it oversamples using the feature vectors from TF-IDF scores computed. Nonetheless, our accuracy is pretty good, at over 95% each run. The classifier usually falls short for the "Both" category.
  
  - Right now, we have a multi-class classifier--it predicts Pain, OUD, or Both. **One potential next step is to edit the classifier to be a binary classifier and produce predictions for Pain and OUD individually--then, we could code for the "Both" category manually by looking for which rows are "Yes" in both categories/columns.** Pain and OUD would be their own Y/N columns, as well as Both. Compare accuracy with original classifier. **I have prepped the data for this next step by splitting the HEAL primary outcome columns of ones and zeros into two columns for Pain and OUD.** 
  
  - Another limitation was that we used additional NIDA codes for the OUD category, but we need a way to label this data as either OUD or both. Right now, the classifier is operating under the assumption that none of this data falls under the category Both, and all of it is ONLY OUD. For the classifier to be more accurate, we need to create labels for that "Both" category within the NIDA dataset, too. Now, if we split the data as mentioned above, we could run the pain classifier on this NIDA data and take those predictions to label the selected dataset as "Both," and re-run our multi-class classifier on this to see how the accuracy performs. However, using this labeling method for the NIDA dataset is risky because it's not validated data by the Think Tank--it's classifier predicted.  
  
  - Future research should perform bagging on the science type classifiers so that the output of one is the input of another. So, for example, once a study is flagged as Basic, Translational, Clinical or Implementation research, only run the specified science type ML classifiers on it next. The image of this bagging is included under `term_sets/Science_Type/`.  Right now, the machine learning classifiers are labeling all science types at once. But this type of bagging may be more effective for classification. However, it is much more efficient to use machine learning to build this upper branch dividing studies into clinical, basic, translational or implementation studies. Then, the machine learning science-type classifiers can be run on the remaining studies for each branch. **This should be a next step.**

- `combine.py` - combined NLP and ML results to find where those models agree vs. disagree. Trust the labels where they both agree--double-check the mismatches. Final label predictions saved to `preds_final` under `results/`. This file is not perfect--it needs work.
 
- `unsupervised.py` - unsupervised machine learning approaches to uncover potential trends/patterns in the textual data. If we want to add more words/narrow down our regular expression searches, it may be helpful to also take a look at the words with the highest TFIDF scores per category for each of our variables. This is what unsupervised machine learning looks like. The computed scores are called the term-frequency inverse-document frequency scores. They are calculated by taking the normalized counts of each word over a series of documents and dividing this count by the number of documents the word actually appears in. This helps us understand what words are the most relevant/significant to a text or label. The higher the TFIDF score, the more relevant/important a word is in a document. These scores are essentially also what train our classifiers. This file helps take a look at the top 15 words per category of each of our variables. Next steps may be modifying our key-word ontology searches to include less word-junk and more distinctive words based on these results. The results are saved under `term_sets/`
